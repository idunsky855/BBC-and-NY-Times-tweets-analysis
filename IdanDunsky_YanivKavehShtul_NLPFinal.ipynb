{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP Final Project \n",
        "\n",
        "### By: Idan Dunsky, Yaniv Kaveh-Shtul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from ntscraper import Nitter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scraper = Nitter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A37tIyy2L4U0"
      },
      "outputs": [],
      "source": [
        "def get_tweets(name,modes,no):\n",
        "  \"\"\" get No. tweets from a specified user, by mode \n",
        "\n",
        "  Args:\n",
        "      name (str): username\n",
        "      modes (str): 1 of hashtag, user, term\n",
        "      no (int): number of tweets to get\n",
        "\n",
        "  Returns:\n",
        "      DataFrame: a Dataframe containing all tweets\n",
        "  \"\"\"\n",
        "  final_tweets = []\n",
        "\n",
        "  tweets = scraper.get_tweets(name, mode = modes, number = no)\n",
        "\n",
        "  for tweet in tweets['tweets']:\n",
        "    data = [tweet['link'],tweet['text'],tweet['date'],tweet['stats']['likes'],tweet['stats']['comments']]\n",
        "    final_tweets.append(data)\n",
        "  data = pd.DataFrame(final_tweets, columns=['link', 'text','date','No_of_Likes','No_of_tweets'])\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mqZKMcaL8x8",
        "outputId": "540d96d1-599f-4d88-c8db-7bd06f8d42fe"
      },
      "outputs": [],
      "source": [
        "BBC = 'BBCNews'    # BBC news twitter account\n",
        "NYTIMES = 'nytimes' # New York Times twitter account\n",
        "MODE = 'user'       # scraping mode\n",
        "NUM_OF_TWEETS = 900 # Maximun num of tweets allowed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_df = get_tweets(BBC, MODE, NUM_OF_TWEETS)\n",
        "nytimes_df = get_tweets(NYTIMES, MODE, NUM_OF_TWEETS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "34z08bV6M2IN",
        "outputId": "ed70a059-0d65-4fb8-8476-87072706ccfa"
      },
      "outputs": [],
      "source": [
        "bbc_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nytimes_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving data to files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePyz6V52PMc2"
      },
      "outputs": [],
      "source": [
        "bbc_df.to_csv(\"bbc_tweets.csv\")\n",
        "nytimes_df.to_csv(\"nytimes_tweets.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BBC = './bbc_tweets.csv'\n",
        "NYT = './nytimes_tweets.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_df = pd.read_csv(BBC)\n",
        "nyt_df = pd.read_csv(NYT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "bbc_corpus = [re.sub(r'http\\S+', '', x) for x in bbc_df['text']]\n",
        "nyt_corpus = [re.sub(r'http\\S+', '', x) for x in nyt_df['text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nyt_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pre-processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nlp model that will perform all actions\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper function to tokenize corpus \n",
        "def process(nlp, corpus):\n",
        "    doc = [nlp(sent) for sent in corpus]\n",
        "    return doc\n",
        "\n",
        "def tokenize(processed_corpus):\n",
        "    tokens = []\n",
        "    for sent in processed_corpus:\n",
        "        tokens.append([token for token in sent if not token.is_punct and not token.is_stop and not token.is_space])\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_processed_corpus = process(nlp, bbc_corpus)\n",
        "nyt_processed_corpus = process(nlp, nyt_corpus)\n",
        "\n",
        "bbc_tokens = tokenize(bbc_processed_corpus)\n",
        "nyt_tokens = tokenize(nyt_processed_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatize(tokens):\n",
        "    lemmas = []\n",
        "    for sent in tokens:\n",
        "        for token in sent:\n",
        "            lemmas.append(token.lemma_)\n",
        "    return set(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_lemmas = lemmatize(bbc_tokens)\n",
        "nyt_lemmas = lemmatize(nyt_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nyt_lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Most frequent words\n",
        "\n",
        "we will now use the naive manual way to search the most frequent words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "def get_most_frequent_words(corpus, top_n=5):\n",
        "    \"\"\"Seeks and returns a list the top_n most common words in a given corpus\n",
        "\n",
        "    Args:\n",
        "        corpus (list[str]): corpus\n",
        "        top_n (int, optional): number of frequent words to seek. Defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: list of most frequent words sorted in an descending order \n",
        "    \"\"\"\n",
        "    # Combine all documents into one string\n",
        "    combined_text = ' '.join(corpus)\n",
        "    \n",
        "    # Tokenize the combined text (split by whitespace and remove non-alphanumeric characters)\n",
        "    words = re.findall(r'\\b\\w+\\b', combined_text.lower())\n",
        "    \n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_counts = Counter(filtered_words)\n",
        "    \n",
        "    # Get the top N most common words\n",
        "    most_common_words = word_counts.most_common(top_n)\n",
        "    \n",
        "    return most_common_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def most_freq(corpus, name, number = 5):\n",
        "    \"\"\"print the {number} most frequent words in a given corpus \n",
        "\n",
        "    Args:\n",
        "        corpus (list[str]): corpus\n",
        "        name (str): corpus name\n",
        "        number (int): number of words to print\n",
        "    \"\"\"\n",
        "    # Get the number most frequent words\n",
        "    top_words = get_most_frequent_words(corpus, top_n=number)\n",
        "\n",
        "    # Display the results\n",
        "    print(f\"Top 5 most frequent words in the {name}:\")\n",
        "    for word, freq in top_words:\n",
        "        print(f\"{word}: {freq}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "most_freq(bbc_corpus, 'BBC')\n",
        "print()\n",
        "most_freq(nyt_corpus,'New-York Times')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF most frequent\n",
        "\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It combines two components: Term Frequency (TF), which measures how often a word appears in a document, and Inverse Document Frequency (IDF), which measures how common or rare a word is across the entire corpus. TF-IDF helps in identifying key terms and improving text analysis tasks like information retrieval and document classification.\n",
        "\n",
        "\n",
        "we will now use the `TfidfVectorizer` in order to find the most frequent words in the corpus.\n",
        "\n",
        "we will extract the sum of `tf` for each word by dividing the results of `TfidfVectorizer` by the `TfidfVectorizer.idf_` score, achieving the frequency of each word in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def tf_idf(corpus: list[str]):\n",
        "    \"\"\"calculates the tfidf score for each word in a given corpus\n",
        "\n",
        "    Args:\n",
        "        corpus (list[str]): the corpus for tfidf calculation\n",
        "\n",
        "    Returns:\n",
        "        tfidf_df DataFrame: data frame that holds the tfidf scores sum for each word \n",
        "    \"\"\"\n",
        "    # Initialize the vectorizer\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # Fit and transform the corpus\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Get feature names (words)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Sum the TF scores for each word across all documents\n",
        "    tf_scores = np.sum(X.toarray()/vectorizer.idf_, axis=0)\n",
        "\n",
        "    # Create a DataFrame for better visualization\n",
        "    tf_df = pd.DataFrame({'word': feature_names, 'tf_score': tf_scores})\n",
        "\n",
        "    # Sort the DataFrame by TF-IDF score in descending order\n",
        "    tf_df = tf_df.sort_values(by='tf_score', ascending=False)\n",
        "\n",
        "    # Display the most frequent words based on TF-IDF scores\n",
        "    return tf_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_tf = tf_idf(bbc_corpus)\n",
        "nyt_tf = tf_idf(nyt_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbc_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nyt_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Frequency Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tfidf_bar_chart(tf_df1, tf_df2, top_n=10):\n",
        "    \"\"\" plot a bar chart of the top_n words with the highest tf score in two corpora\n",
        "    Args:\n",
        "        tf_df1 (DataFrame): tfidf dataframe for corpus #1 \n",
        "        tf_df2 (DataFrame): tfidf dataframe for corpus #2 \n",
        "        top_n (int, optional): number of words to plot. Defaults to 10.\n",
        "    \"\"\"\n",
        "    # Get the top N words by TF score for each DataFrame\n",
        "    top_tf_df1 = tf_df1.head(top_n)\n",
        "    top_tf_df2 = tf_df2.head(top_n)\n",
        "    \n",
        "    # Combine both DataFrames for plotting\n",
        "    combined_df = pd.concat([top_tf_df1, top_tf_df2])\n",
        "    \n",
        "    # Mark the source of each word for color coding\n",
        "    combined_df['source'] = ['DF1']*top_n + ['DF2']*top_n\n",
        "\n",
        "    # Sort combined DataFrame by TF-IDF score\n",
        "    combined_df = combined_df.sort_values(by='tf_score', ascending=False)\n",
        "\n",
        "    # Create a bar chart\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    colors = ['skyblue' if source == 'DF1' else 'salmon' for source in combined_df['source']]\n",
        "    plt.barh(combined_df['word'], combined_df['tf_score'], color=colors)\n",
        "    plt.xlabel('TF Score')\n",
        "    plt.title(f'Top {top_n} Words by TF Score')\n",
        "    plt.gca().invert_yaxis()\n",
        "\n",
        "    # Create a legend\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [Line2D([0], [0], color='skyblue', lw=4, label='BBC'),\n",
        "                       Line2D([0], [0], color='salmon', lw=4, label='New-York Times')]\n",
        "    plt.legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_tfidf_bar_chart(bbc_tf, nyt_tf, top_n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merged_df = pd.merge(bbc_tf,nyt_tf, on='word', suffixes=('_bbc', '_nyt'))\n",
        "# merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word Cloud Frequency Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_word_clouds(tf_df1, tf_df2, top_n=5):\n",
        "    # Get the top N words by TF score for each DataFrame\n",
        "    top_tf_df1 = tf_df1.head(top_n)\n",
        "    top_tf_df2 = tf_df2.head(top_n)\n",
        "    \n",
        "    # Create dictionaries for word cloud generation\n",
        "    word_freq1 = dict(zip(top_tf_df1['word'], top_tf_df1['tf_score']))\n",
        "    word_freq2 = dict(zip(top_tf_df2['word'], top_tf_df2['tf_score']))\n",
        "    \n",
        "    # Create word clouds\n",
        "    wordcloud1 = WordCloud(width=800, height=400, background_color='white', colormap='Blues').generate_from_frequencies(word_freq1)\n",
        "    wordcloud2 = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate_from_frequencies(word_freq2)\n",
        "\n",
        "    # Create word clouds with borders\n",
        "    wordcloud1 = WordCloud(width=800, height=400, background_color='white', contour_color='black', contour_width=10).generate_from_frequencies(word_freq1)\n",
        "    wordcloud2 = WordCloud(width=800, height=400, background_color='white', contour_color='black', contour_width=10).generate_from_frequencies(word_freq2)\n",
        "    \n",
        "    # Plot word clouds\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(wordcloud1, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'{top_n} Top Words in BBC')\n",
        "    \n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(wordcloud2, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'{top_n} Top Words in New-York Times')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_word_clouds(bbc_tf, nyt_tf, top_n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Frequency Extraction using Word2Vec\n",
        "\n",
        "Word2Vec is a type of neural network model used to learn vector representations of words from large text corpora. It operates in two main architectures: Continuous Bag of Words (CBOW) and Skip-gram. CBOW predicts a target word from its surrounding context words, while Skip-gram predicts context words from a given target word. These vector representations capture semantic meanings and relationships between words, making Word2Vec useful for tasks such as word similarity, sentiment analysis, and language translation.\n",
        "\n",
        "Using Word2Vec to find the most frequent words in a corpus is not its primary function, as Word2Vec is designed to create word embeddings based on the context in which words appear rather than to count word frequencies. However, you can still retrieve the most frequent words from the vocabulary built during the training of a Word2Vec model.\n",
        "\n",
        "### Explanation:\n",
        "- Training the Model: The Word2Vec model is trained on the corpus to create word embeddings.\n",
        "- Vocabulary and Counts: The key_to_index attribute of the model's wv (word vectors) object provides access to the vocabulary. The get_vecattr method retrieves the count of each word.\n",
        "- Sorting and Displaying: The words are sorted by their counts in descending order, and the top N most frequent words are displayed.\n",
        "This method leverages the internal vocabulary built during the Word2Vec training process to find the most frequent words in the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_n_frequent_words(token_list, n=10, vector_size=100, window=5, min_count=1, workers=4):\n",
        "    \"\"\"\n",
        "    Train a Word2Vec model on the given corpus and return the top N most frequent words.\n",
        "\n",
        "    Parameters:\n",
        "    - corpus: List of List of tokenized sentences (list of list of strings).\n",
        "    - n: Number of top frequent words to return.\n",
        "    - vector_size: Size of the word vectors.\n",
        "    - window: Maximum distance between the current and predicted word within a sentence.\n",
        "    - min_count: Ignores all words with a total frequency lower than this.\n",
        "    - workers: Number of worker threads to train the model.\n",
        "\n",
        "    Returns:\n",
        "    - List of tuples (word, count) for the top N most frequent words.\n",
        "    - Trained Word2Vec model\n",
        "    \"\"\"\n",
        "    # Train the Word2Vec model\n",
        "    model = Word2Vec(sentences=[[token.text for token in doc] for doc in token_list], vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
        "    \n",
        "    # Get the vocabulary and their counts\n",
        "    vocab = model.wv.key_to_index\n",
        "    word_counts = {word: model.wv.get_vecattr(word, \"count\") for word in vocab}\n",
        "    \n",
        "    # Sort words by their frequency (count)\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "    \n",
        "    # Get the top N most frequent words\n",
        "    most_frequent_words = sorted_word_counts[:n]\n",
        "    \n",
        "    return most_frequent_words, model\n",
        "\n",
        "\n",
        "def print_top_n_frequent_words(token_list, top_n=5):\n",
        "    \"\"\" Get top_n most frequent words using Word2Vec model, and print it.\n",
        "\n",
        "    Parameters:\n",
        "        - token_list: List of tokenized words (list of strings) \n",
        "        - top_n: number of words to print. Defaults to 5. \n",
        "    \n",
        "    Returns:\n",
        "        - Trained Word2Vec model\n",
        "    \"\"\"\n",
        "    top_words, model = get_top_n_frequent_words(token_list, n=top_n)\n",
        "    \n",
        "    print(f\"\\nTop {top_n} Most Frequent Words:\")\n",
        "    for word, count in top_words:\n",
        "        print(f'Word: {word}, Count: {count}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w2v_model_bbc = print_top_n_frequent_words(bbc_tokens, top_n=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w2v_model_nyt = print_top_n_frequent_words(nyt_tokens, top_n=15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoEncoder Significance Extraction\n",
        "\n",
        "An autoencoder is a type of neural network designed for unsupervised learning that compresses input data into a lower-dimensional representation and then reconstructs it. It consists of two main parts: an encoder that reduces the data's dimensionality and a decoder that reconstructs the original data from the compressed form. Autoencoders are used for tasks such as dimensionality reduction, feature learning, and anomaly detection.\n",
        "\n",
        "While autoencoders are not typically used for identifying the most significant words in a corpus, it is possible to adapt them for this purpose with some modifications.\n",
        "\n",
        "\n",
        "### Explanation:\n",
        "- We start by tokenizing our corpus and training a Word2Vec model to get word embeddings.\n",
        "- We create an autoencoder with an input layer, a hidden layer (encoding), and an output layer (decoding).\n",
        "- The autoencoder is trained to reconstruct the word embeddings.\n",
        "- After training, we use the autoencoder to reconstruct the embeddings and calculate the reconstruction error for each word.\n",
        "- Words with higher reconstruction errors are considered potentially more significant, as they might be harder for the autoencoder to encode and decode accurately.\n",
        "- We sort the words based on their reconstruction error and print the top 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autoencoder_significance_analysis(w2v_model, tokenized_corpus, top_n=10):\n",
        "    \n",
        "    # Create word embeddings for each word in the corpus\n",
        "    word_embeddings = {}\n",
        "    for doc in tokenized_corpus:\n",
        "        for word in doc:\n",
        "            if word not in word_embeddings:\n",
        "                word_embeddings[word.text] = w2v_model.wv[word.text]\n",
        "\n",
        "    # Convert word embeddings to a list\n",
        "    embedding_list = list(word_embeddings.values())\n",
        "    embedding_matrix = np.array(embedding_list)\n",
        "\n",
        "    # Define the autoencoder\n",
        "    input_dim = embedding_matrix.shape[1]\n",
        "    encoding_dim = 32\n",
        "\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "    decoded = Dense(input_dim, activation='linear')(encoded)\n",
        "\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the autoencoder\n",
        "    autoencoder.fit(embedding_matrix, embedding_matrix, epochs=100, batch_size=16, shuffle=True, verbose=0)\n",
        "\n",
        "    # Get the reconstructed embeddings\n",
        "    reconstructed_embeddings = autoencoder.predict(embedding_matrix)\n",
        "\n",
        "    # Calculate reconstruction error for each word\n",
        "    reconstruction_errors = np.mean(np.square(embedding_matrix - reconstructed_embeddings), axis=1)\n",
        "\n",
        "    # Create a dictionary of words and their reconstruction errors\n",
        "    word_errors = {word: error for word, error in zip(word_embeddings.keys(), reconstruction_errors)}\n",
        "\n",
        "    # Sort words by reconstruction error (higher error might indicate more significant words)\n",
        "    sorted_words = sorted(word_errors.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print the top 10 words with highest reconstruction error\n",
        "    print(f\"\\nTop {top_n} potentially significant words based on reconstruction error:\\n\")\n",
        "    for word, error in sorted_words[:top_n]:\n",
        "        print(f\"{word}: {error}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "autoencoder_significance_analysis(w2v_model_bbc, bbc_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "autoencoder_significance_analysis(w2v_model_nyt, nyt_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##################################### SKIPPED SOME STUFF ( COMPARISON ) ############################################ \n",
        "\n",
        "##################################### SKIPPED SOME STUFF ( COMPARISON ) ############################################ \n",
        "\n",
        "##################################### SKIPPED SOME STUFF ( COMPARISON ) ############################################ \n",
        "\n",
        "##################################### SKIPPED SOME STUFF ( COMPARISON ) ############################################ \n",
        "\n",
        "##################################### SKIPPED SOME STUFF ( COMPARISON ) ############################################ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NER extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_ents(doc):\n",
        "    if doc.ents:\n",
        "        for ent in doc.ents:\n",
        "            print(f\"Word: {ent.text: <35} NER: {ent.label_: <35} Explanation:\" +  str(spacy.explain(ent.label_)))\n",
        "    else:\n",
        "        print(\"No named entities found.\")\n",
        "        \n",
        "def print_NER(corpus):\n",
        "    for i in range(len(corpus)):\n",
        "        show_ents(corpus[i])\n",
        "    \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_NER(bbc_processed_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
